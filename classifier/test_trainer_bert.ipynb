{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 13497\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5064\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='5064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/5064 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-500\n",
      "Configuration saved in output\\checkpoint-500\\config.json\n",
      "Model weights saved in output\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-1000\n",
      "Configuration saved in output\\checkpoint-1000\\config.json\n",
      "Model weights saved in output\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-1500\n",
      "Configuration saved in output\\checkpoint-1500\\config.json\n",
      "Model weights saved in output\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-2000\n",
      "Configuration saved in output\\checkpoint-2000\\config.json\n",
      "Model weights saved in output\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-2500\n",
      "Configuration saved in output\\checkpoint-2500\\config.json\n",
      "Model weights saved in output\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-3000\n",
      "Configuration saved in output\\checkpoint-3000\\config.json\n",
      "Model weights saved in output\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-3500\n",
      "Configuration saved in output\\checkpoint-3500\\config.json\n",
      "Model weights saved in output\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-4000\n",
      "Configuration saved in output\\checkpoint-4000\\config.json\n",
      "Model weights saved in output\\checkpoint-4000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-4500\n",
      "Configuration saved in output\\checkpoint-4500\\config.json\n",
      "Model weights saved in output\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-5000\n",
      "Configuration saved in output\\checkpoint-5000\\config.json\n",
      "Model weights saved in output\\checkpoint-5000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output\\checkpoint-3000 (score: 0.9072011709213257).\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\eliaf/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\eliaf/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\eliaf/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\eliaf/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file output/checkpoint-5000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/checkpoint-5000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from classifier_trainer_bert import Dataset, clean_txt\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "PATH = \"../data/binary/us-politicians-with-clusters.pickle\"\n",
    "test_data = pd.read_pickle(PATH)\n",
    "test_data['quotation'] = test_data['quotation'].apply(clean_txt)\n",
    "X_test = list(test_data[\"quotation\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"output/checkpoint-5000\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=7)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1650105\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='206264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [     1/206264 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "# Preprocess raw predictions\n",
    "y = np.argmax(raw_pred, axis=1)\n",
    "# 1: External Relations\n",
    "# 2: Freedom and Democracy\n",
    "# 3: Political System\n",
    "# 4: Economy\n",
    "# 5: Welfare and quality of life\n",
    "# 6: Fabric of Society\n",
    "# 7: Social Groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)\n",
    "test_data['manifesto_topic'] = y\n",
    "topic_map = {0:\"External Relations\", 1: \"Freedom and Democracy\", 2: \"Political System\", 3: \"Economy\", 4: \"Welfare and quality of life\", 5: \"Fabric of Society\", 6: \"Social Groups\"}\n",
    "test_data['topic'] = test_data['topic'].map(topic_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(test_data['manifesto_topic'], bins = 7)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-953d23d9",
   "language": "python",
   "display_name": "PyCharm (ada-2021-project-adaboy)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}