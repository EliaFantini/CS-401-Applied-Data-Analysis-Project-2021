{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   speaker_id           quote_id  \\\n0     1282411  2015-06-11-000090   \n1     1282411  2015-08-11-090504   \n2     1282411  2015-10-27-125840   \n3     1282411  2015-01-19-008025   \n4     1282411  2015-12-09-094134   \n\n                                           quotation    speaker  party  \\\n0  25-year cost estimate for the U.S. nuclear mod...  Ed Markey      1   \n1  The state is transitioning to a higher percent...  Ed Markey      1   \n2  This highly coveted TIGER grant funding would ...  Ed Markey      1   \n3  carefully cutting the Gordian knot that has ti...  Ed Markey      1   \n4  The amendments would improve the safety of agi...  Ed Markey      1   \n\n   Date-Time  \n0 2015-06-11  \n1 2015-08-11  \n2 2015-10-27  \n3 2015-01-19  \n4 2015-12-09  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker_id</th>\n      <th>quote_id</th>\n      <th>quotation</th>\n      <th>speaker</th>\n      <th>party</th>\n      <th>Date-Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1282411</td>\n      <td>2015-06-11-000090</td>\n      <td>25-year cost estimate for the U.S. nuclear mod...</td>\n      <td>Ed Markey</td>\n      <td>1</td>\n      <td>2015-06-11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1282411</td>\n      <td>2015-08-11-090504</td>\n      <td>The state is transitioning to a higher percent...</td>\n      <td>Ed Markey</td>\n      <td>1</td>\n      <td>2015-08-11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1282411</td>\n      <td>2015-10-27-125840</td>\n      <td>This highly coveted TIGER grant funding would ...</td>\n      <td>Ed Markey</td>\n      <td>1</td>\n      <td>2015-10-27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1282411</td>\n      <td>2015-01-19-008025</td>\n      <td>carefully cutting the Gordian knot that has ti...</td>\n      <td>Ed Markey</td>\n      <td>1</td>\n      <td>2015-01-19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1282411</td>\n      <td>2015-12-09-094134</td>\n      <td>The amendments would improve the safety of agi...</td>\n      <td>Ed Markey</td>\n      <td>1</td>\n      <td>2015-12-09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pickle of us politicians and \n",
    "POLITICIANS_PICKLE_PATH = \"../data/binary/us-politicians.pickle\"\n",
    "politicians = pd.read_pickle(POLITICIANS_PICKLE_PATH)\n",
    "politicians.loc[politicians['party'] == 29552, 'party'] = 1\n",
    "politicians.loc[politicians['party'] == 29468, 'party'] = 0\n",
    "politicians.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                             sentence  label\n7             Our great middle class is hard-pressed.    704\n8   Millions of Americans have lost their jobs, an...    704\n14  We have a plan to build a strong, growing econ...    404\n16  And we will honor the values of a strong Ameri...    201\n24  Alone among nations, America was born in pursu...    201",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>Our great middle class is hard-pressed.</td>\n      <td>704</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Millions of Americans have lost their jobs, an...</td>\n      <td>704</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>We have a plan to build a strong, growing econ...</td>\n      <td>404</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>And we will honor the values of a strong Ameri...</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Alone among nations, America was born in pursu...</td>\n      <td>201</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pickle of labels\n",
    "LABELS_CSV_PATH = \"../classifier/data/join_result.csv\"\n",
    "labels = pd.read_csv(LABELS_CSV_PATH)\n",
    "labels = labels.drop(labels[labels['label'] == 0].index)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<function matplotlib.pyplot.show(close=None, block=None)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Division by topic:\n",
    "# 1: External Relations\n",
    "# 2: Freedom and Democracy\n",
    "# 3: Political System\n",
    "# 4: Economy\n",
    "# 5: Welfare and quality of life\n",
    "# 6: Fabric of Society\n",
    "# 7: Social Groups\n",
    "topics = labels.copy()\n",
    "\n",
    "for i in range(1, 8):\n",
    "    topics.loc[ topics['label']//100 == i , 'label'] = i \n",
    "\n",
    "plt.hist(topics['label'], bins = 7)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                             sentence  label\n7             Our great middle class is hard-pressed.      7\n8   Millions of Americans have lost their jobs, an...      7\n14  We have a plan to build a strong, growing econ...      4\n16  And we will honor the values of a strong Ameri...      2\n24  Alone among nations, America was born in pursu...      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>Our great middle class is hard-pressed.</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Millions of Americans have lost their jobs, an...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>We have a plan to build a strong, growing econ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>And we will honor the values of a strong Ameri...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Alone among nations, America was born in pursu...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Division by left/right:\n",
    "# 0. Left\n",
    "# 1. Right\n",
    "labelLeftRight = {\n",
    "    'right': [104,201,203,305,401,402,407,414,505,601,603,605,606],\n",
    "    'left': [103,105,106,107,403,404,406,412,413,504,506,701,202]\n",
    "    }\n",
    "leftRight = labels.copy()\n",
    "leftRight.loc[ leftRight['label'].isin(labelLeftRight['right']), 'label'] = 1\n",
    "leftRight.loc[ leftRight['label'].isin(labelLeftRight['left']), 'label'] = 0\n",
    "leftRight = leftRight.drop(leftRight[(leftRight['label'] != 0) & (leftRight['label'] != 1)].index)\n",
    "\n",
    "plt.hist(leftRight['label'], bins = 2)\n",
    "plt.show\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(txt):\n",
    "    txt = re.sub(\"'\",\"\",txt)\n",
    "    txt = re.sub(\"(\\\\W)+\",\" \",txt)\n",
    "    return txt\n",
    "\n",
    "topics['sentence'] = topics['sentence'].apply(clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = list(df['label'])\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True,return_tensors=\"pt\") for text in df['sentence']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11997 1500 1500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(topics.sample(frac=1, random_state=42),\n",
    "                                     [int(.8*len(topics)), int(.9*len(topics))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/5999 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"host_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-17-caa4c7221417>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[0mLR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1e-6\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mLR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     72\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-17-caa4c7221417>\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model_bert, train_data, val_data, learning_rate, epochs)\u001B[0m\n\u001B[0;32m     29\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_bert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m                 \u001B[0mbatch_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_label\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m                 \u001B[0mtotal_loss_train\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mbatch_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1150\u001B[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0;32m   1151\u001B[0m                                \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1152\u001B[1;33m                                label_smoothing=self.label_smoothing)\n\u001B[0m\u001B[0;32m   1153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   2844\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2845\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2846\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcross_entropy_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_smoothing\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2847\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2848\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: \"host_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "def train_model(model_bert, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model_bert.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model_bert = model_bert.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model_bert(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output.long(), train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model_bert.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model_bert(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    a=23\n",
    "                    total_loss_val += batch_loss.item()\n",
    "\n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "\n",
    "train_model(model, df_train, df_val, LR, EPOCHS)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "pycharm-8f20d7e5",
   "language": "python",
   "display_name": "PyCharm (ml-project-1-pasta_balalaika)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}