{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import textstat as ts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import bz2\n",
    "from statistics import mode\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting politicians dataset\n",
    "To obtain a dataset of politicians' quotations, we need to enrich the original *Quotebank* dataset with additional information about the speakers' political affiliations. To achieve that, we propose a preprocessing pipeline.\n",
    "\n",
    "First, load the *Wikidata* table from the `.parquet` file (provided on the *Quotebank*'s Google Drive). Keep the columns containing information interesting for us - speaker's QID and label, and party's QID, discard the others and drop the rows with missing values. Convert that to a `pandas DataFrame` and dump it into a `.pickle` file. \n",
    "\n",
    "Then, for each of the years load the initial *Quotebank* data. The initial dataset is represented as a list of `json` objects - process it into a `DataFrame` in a line-by-line manner, only keeping the quotation content and QID, as well as the speaker's QID. Dump the dataframe into a `.pickle` file.\n",
    "\n",
    "Next, perform an inner join between the two dataframes on the speaker QID - that way you end up with a dataframe containing quotations, along with information about the speaker's political affiliation.\n",
    "\n",
    "The *Quotebank* dataset is divided into batches based on the quotation date - 6 files are corresponding to years from 2015 until 2020. Therefore we perform the pipeline explained above 6 times, and then merge the results into a single dataframe. We end up with a dataset of around 17m quotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speaker_affiliations(parquet_path, out_path, remove_raw=False):\n",
    "\n",
    "    # load speaker info\n",
    "    speaker_info = pd.read_parquet(parquet_path)\n",
    "    speaker_info = speaker_info[[\"id\", \"label\", \"party\"]]\n",
    "\n",
    "    # take the speakers that have an assigned political affiliation\n",
    "    speaker_info = speaker_info.dropna()\n",
    "\n",
    "    # in case of multiple affiliations, take the first affiliation only\n",
    "    # speaker_info[\"party\"] = speaker_info[\"party\"].apply(lambda x: int(x[0][1:]))\n",
    "\n",
    "    # alternatively (I think a slightly better way), select most common party\n",
    "    speaker_info[\"party\"] = speaker_info[\"party\"].apply(lambda x: mode(x)[1:])\n",
    "\n",
    "    # transform speaker id into int\n",
    "    speaker_info[\"id\"] = speaker_info[\"id\"].apply(lambda x: int(x[1:]))\n",
    "    \n",
    "    print(f\"Speaker affiliation DF:\\n {speaker_info.head()}\")\n",
    "\n",
    "    speaker_info.to_pickle(out_path)\n",
    "\n",
    "    if remove_raw:\n",
    "        os.remove(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(json_path_bz2, pickle_path, remove_raw=False):\n",
    "    data = [] \n",
    "    with bz2.open(json_path_bz2, 'rb') as s_file:\n",
    "        print(\"Quotation file opened...\")\n",
    "        for instance in tqdm(s_file):\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            \n",
    "            # if there is no speaker, skip current row\n",
    "            if not instance['qids']:\n",
    "                continue\n",
    "            \n",
    "            # else proceed to read the data\n",
    "            row = dict()\n",
    "            row['speaker_id'] = int(instance['qids'][0][1:])\n",
    "            row['quote_id'] = instance['quoteID']\n",
    "            row['quotation'] = instance['quotation']\n",
    "            data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_pickle(pickle_path)\n",
    "    \n",
    "    if remove_raw:\n",
    "        os.remove(json_path_bz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_quotes_with_speaker_affiliations(df_quotes, df_affiliations, out_path):\n",
    "    # join the quote data with their corresponding labels\n",
    "    merged = pd.merge(left=df_quotes, left_on=\"speaker_id\", right=df_affiliations, right_on=\"id\")\n",
    "    merged = merged.drop(columns=[\"id\"])\n",
    "    merged = merged.rename(columns = {\"label\": \"speaker\"})\n",
    "    print(f\"Merged DF: \\n{merged.head()}\")\n",
    "    merged.to_pickle(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time operation - generate a pickle file containing speaker's affiliations\n",
    "PARQUET_PATH = \"../data/raw/speaker_attributes.parquet\"\n",
    "SPEAKER_AFFILIATIONS_OUT_PATH = \"../data/binary/speaker_attributes.pickle\"\n",
    "\n",
    "print(\"Generating speaker affiliations DF...\")\n",
    "if not os.path.exists(SPEAKER_AFFILIATIONS_OUT_PATH):\n",
    "    generate_speaker_affiliations(PARQUET_PATH, SPEAKER_AFFILIATIONS_OUT_PATH)\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "# dataset loading - perform for each batch of the data (2015, 2016, ..., 2020)\n",
    "years = []\n",
    "for year in years:\n",
    "    DATASET_PATH_JSON_BZ2 = f\"../data/raw/quotes-{year}.json.bz2\"\n",
    "    DATASET_PATH_PICKLE = f\"../data/binary/quotes-{year}.pickle\"\n",
    "    MERGED_OUT_PATH = f\"../data/binary/data-{year}.pickle\"\n",
    "\n",
    "    print(\"Generating quotes DF...\")\n",
    "    if not os.path.exists(DATASET_PATH_PICKLE):\n",
    "        save_pickle(DATASET_PATH_JSON_BZ2, DATASET_PATH_PICKLE)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    df_quotes = pd.read_pickle(DATASET_PATH_PICKLE)\n",
    "    df_affiliations = pd.read_pickle(SPEAKER_AFFILIATIONS_OUT_PATH)\n",
    "    \n",
    "    print(\"Generating merged df...\")\n",
    "    if not os.path.exists(MERGED_OUT_PATH):\n",
    "        join_quotes_with_speaker_affiliations(df_quotes, df_affiliations, MERGED_OUT_PATH)\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the results for the separate years into a single dataframe,\n",
    "# and extracting date from the quote_id and saving it into a new column as datetime.\n",
    "# Save all into a pickle named data.pickle\n",
    "\n",
    "if not os.path.exists(\"../data/binary/data.pickle\"):\n",
    "    years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "    paths = [f\"../data/binary/data-{year}.pickle\" for year in years]\n",
    "    dfs = [pd.read_pickle(path) for path in paths]\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # create an additional datetime column - useful for temporal analysis\n",
    "    merged_df[\"Date-Time\"] = pd.to_datetime(merged_df['quote_id'].apply(lambda x : datetime.strptime(x[:10], '%Y-%m-%d')))\n",
    "    merged_df.to_pickle(\"../data/binary/data.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Politicians dataset\n",
    "To obtain a dataset of US Politician quotations, we proceed to:\n",
    "1. Only keep the quotations where the speaker's party is `29468` (Republican party) or `29552` (Democratic party).\n",
    "2. Filter out the rows that have `None` as a value in the `Candidacy` column in the Wikidata - the majority of the speakers affiliated with the political parties were not actual politicians - they are often celebrities, sports stars, TV personalities, etc. We believe it is beneficial to only take the actual politicians, as they are more likely to speak about actual political matters and represent their party's ideology.\n",
    "\n",
    "Performing step 1 reduces the size of the dataset from 17 million rows to around 8 million rows, and step 2 reduces the size further to around 1.6 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional preprocessing to obtain the US politicans data\n",
    "df = pd.read_pickle(\"../data/binary/data.pickle\")\n",
    "# filter to only keep the american politicians\n",
    "df = df[df[\"party\"].isin([29468, 29552])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain only the quotes where the speakers are actually politicians - they were candidates in at least one election\n",
    "speaker_data = pd.read_parquet(PARQUET_PATH)\n",
    "candidates = speaker_data[[\"id\", \"candidacy\"]]\n",
    "candidates = candidates.dropna()\n",
    "candidates = candidates.drop(columns=[\"candidacy\"])\n",
    "candidates[\"id\"] = candidates[\"id\"].apply(lambda x: int(x[1:]))\n",
    "candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only keep the ones that actually participated in an election (exclude celebrities etc.)\n",
    "df = pd.merge(left=df, left_on=\"speaker_id\", right_on=\"id\", right=candidates)\n",
    "df = df.drop(columns=[\"id\"])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "if not os.path.exists(\"../data/binary/us-politicians.pickle\"):\n",
    "    df.to_pickle(\"../data/binary/us-politicians.pickle\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ca7bf982d50e176929d33c65df232637f1d4c63720dcda24aee19eb37e3e0c5"
  },
  "kernelspec": {
   "display_name": "PyCharm (ml-project-1-pasta_balalaika)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
