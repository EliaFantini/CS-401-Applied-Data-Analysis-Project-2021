{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speaker_affiliations(parquet_path, out_path, remove_raw=False):\n",
    "\n",
    "    # load speaker info\n",
    "    speaker_info = pd.read_parquet(parquet_path)\n",
    "    speaker_info = speaker_info[[\"aliases\", \"party\", \"label\"]]\n",
    "\n",
    "    # take the speakers that have an assigned political affiliation\n",
    "    speaker_info = speaker_info.dropna()\n",
    "\n",
    "    # take the first affiliation only (TODO: this is most likely the best method, think what to do when someone has multiple affiliations)\n",
    "    speaker_info[\"party\"] = speaker_info[\"party\"].apply(lambda x: x[0])\n",
    "\n",
    "    # generate a mapping from all aliases to one single name\n",
    "    speaker_info = speaker_info.explode(\"aliases\")[[\"aliases\", \"label\", \"party\"]]\n",
    "    speaker_info = speaker_info.rename(columns = {\"aliases\": \"speaker\"})\n",
    "    print(f\"Speaker affiliation DF:\\n {speaker_info.head()}\")\n",
    "\n",
    "    speaker_info.to_pickle(out_path)\n",
    "\n",
    "    if remove_raw:\n",
    "        os.remove(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(json_path_bz2, pickle_path, remove_raw=False):\n",
    "    data = [] \n",
    "    columns = ['quoteID', 'quotation', 'speaker']\n",
    "    with bz2.open(json_path_bz2, 'rb') as s_file:\n",
    "        print(\"Quotation file opened...\")\n",
    "        count = 0\n",
    "        for instance in tqdm(s_file):\n",
    "            count += 1\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            if instance['speaker'] == \"None\":\n",
    "                continue\n",
    "            row = dict((k, instance[k]) for k in columns)\n",
    "            data.append(row)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_pickle(pickle_path)\n",
    "    \n",
    "    if remove_raw:\n",
    "        os.remove(json_path_bz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_quotes_with_speaker_affiliations(df_quotes, df_affiliations, out_path):\n",
    "    # join the quote data with their corresponding labels\n",
    "    merged = pd.merge(left=df_quotes, left_on=\"speaker\", right=df_affiliations, right_on=\"speaker\")\n",
    "    merged = merged.drop(columns=[\"speaker\"])\n",
    "    merged = merged.rename(columns = {\"label\": \"speaker\"})\n",
    "    print(f\"Merged DF: \\n{merged.head()}\")\n",
    "    merged.to_pickle(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time operation - generate a pickle file containing speaker's affiliations\n",
    "PARQUET_PATH = \"../data/raw/speaker_attributes.parquet\"\n",
    "SPEAKER_AFFILIATIONS_OUT_PATH = \"../data/binary/speaker_attributes.pickle\"\n",
    "\n",
    "print(\"Generating speaker affiliations DF...\")\n",
    "if not os.path.exists(SPEAKER_AFFILIATIONS_OUT_PATH):\n",
    "    generate_speaker_affiliations(PARQUET_PATH, SPEAKER_AFFILIATIONS_OUT_PATH)\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "# dataset loading - perform for each batch of the data (2015, 2016, ..., 2020)\n",
    "years = [2020]\n",
    "for year in years:\n",
    "    DATASET_PATH_JSON_BZ2 = f\"../data/raw/quotes-{year}.json.bz2\"\n",
    "    DATASET_PATH_PICKLE = f\"../data/binary/quotes-{year}.pickle\"\n",
    "    MERGED_OUT_PATH = f\"../data/binary/data-{year}.pickle\"\n",
    "\n",
    "    print(\"Generating quotes DF...\")\n",
    "    if not os.path.exists(DATASET_PATH_PICKLE):\n",
    "        save_pickle(DATASET_PATH_JSON_BZ2, DATASET_PATH_PICKLE)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    df_quotes = pd.read_pickle(DATASET_PATH_PICKLE)\n",
    "    df_affiliations = pd.read_pickle(SPEAKER_AFFILIATIONS_OUT_PATH)\n",
    "    \n",
    "    print(\"Generating merged df...\")\n",
    "    if not os.path.exists(MERGED_OUT_PATH):\n",
    "        join_quotes_with_speaker_affiliations(df_quotes, df_affiliations, MERGED_OUT_PATH)\n",
    "    print(\"Done.\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ca7bf982d50e176929d33c65df232637f1d4c63720dcda24aee19eb37e3e0c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ada': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
